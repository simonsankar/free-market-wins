---
dg-publish: true
---
Utilitarianism is **a theory of morality that advocates actions that foster happiness and oppose actions that cause unhappiness**. Utilitarianism promotes "the greatest amount of good for the greatest number of people."

It's aim is to maximize the 'utility' when deciding what actions to take given a conflict.

This fails on multiple grounds
- It permits aggression — [[Mixed Law]]
- Fundamentally, attempts to convert ordinal data (people's) to cardinal (ratio level data) — [[The Error of Utilitarianism]]
- It is an ethic based on [[Parasitism]] which has to be based on some prior ethic in which initial value must firstly be derived, meaning it cannot tell us anything on what how we should value things but that we can only attempt to steal from people. It is essentially a stolen concept fallacy like saying property is theft (theft requires the prior concept of property) — so it is saying redistribution of goods/services is good without showing how distribution is to be done in the first place